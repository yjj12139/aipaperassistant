# -*- coding: utf-8 -*-
"""aipaperassistant.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1x2tVhpwBDr-cE96D4BaDyIU2nbiRXPuB
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile aipaperassistant.py

# æ¨¡å—1ï¼šå¯¼å…¥ä¾èµ–åº“
# ========================
#1.1streamlitã€pymupdfã€arxivå®‰è£…å’Œç¯å¢ƒé…ç½®
!pip install streamlit PyMuPDF arxiv
# å®‰è£…è™šæ‹Ÿæ˜¾ç¤ºæ”¯æŒï¼ˆColabæ²¡æœ‰å›¾å½¢ç•Œé¢ï¼‰
!apt-get install -y xvfb
!pip install pyvirtualdisplay
# è®¾ç½®è™šæ‹Ÿæ˜¾ç¤º
from pyvirtualdisplay import Display
display = Display(visible=0, size=(1024, 768))
display.start()
import streamlit as st  # ç½‘é¡µåº”ç”¨æ¡†æ¶
import fitz  # PyMuPDFåº“ï¼Œç”¨äºPDFè§£æ
import numpy as np  # æ•°å€¼è®¡ç®—
import openai  # å¤§æ¨¡å‹APIæ¥å£
import os  # æ“ä½œç³»ç»Ÿæ¥å£
import re  # æ­£åˆ™è¡¨è¾¾å¼
import time  # æ—¶é—´å¤„ç†
from wordcloud import WordCloud  # è¯äº‘ç”Ÿæˆ
import matplotlib.pyplot as plt  # æ•°æ®å¯è§†åŒ–
from sentence_transformers import SentenceTransformer  # è¯­ä¹‰æœç´¢æ¨¡å‹
from sklearn.metrics.pairwise import cosine_similarity  # ç›¸ä¼¼åº¦è®¡ç®—
import arxiv  # arXivè®ºæ–‡è·å–

# æ¨¡å—2ï¼šåˆå§‹åŒ–è®¾ç½®
# ========================
# é…ç½®Streamlité¡µé¢
st.set_page_config(
    page_title="ğŸ“š AIæ–‡çŒ®é˜…è¯»ç³»ç»Ÿ",
    page_icon="ğŸ“–",
    layout="wide")
# åœ¨streamlité…ç½®DeepSeek API
openai.api_key = st.secrets["DEEPSEEK_API_KEY"]

# æ¨¡å—3ï¼šPDFè§£æç±»
# ========================
class PDFProcessor:
    """å¤„ç†PDFæ–‡ä»¶ï¼Œæå–æ–‡æœ¬å’Œä½ç½®ä¿¡æ¯"""
    def __init__(self):
        self.text = ""
        self.annotations = []
        self.title = "æœªå‘½åæ–‡çŒ®"

    def parse_pdf(self, file_path):
        """è§£æPDFæ–‡æœ¬å’Œä½ç½®ä¿¡æ¯"""
        try:
            doc = fitz.open(file_path)  # æ‰“å¼€PDFæ–‡ä»¶
            full_text = ""
            annotations = []

            # é€é¡µè§£æ
            for page_num in range(len(doc)):
                page = doc.load_page(page_num)  # åŠ è½½å½“å‰é¡µ
                blocks = page.get_text("dict").get("blocks", [])  # è·å–æ–‡æœ¬å—

                # éå†æ–‡æœ¬å—
                #PDFæ–‡æœ¬æœ‰å±‚çº§ç»“æ„ï¼šå—(block) â†’ è¡Œ(line) â†’ ç‰‡æ®µ(span)
                for b in blocks:
                    if "lines" in b:  # ç¡®è®¤æ˜¯æ–‡æœ¬è¡Œï¼ˆæ¯ä¸ªæ–‡æœ¬å—blocksåŒ…å«è‡ªå·±çš„linesåˆ—è¡¨ï¼‰
                    #æ’é™¤å›¾åƒå—ã€ç©ºç™½å—ç­‰éæ–‡æœ¬å†…å®¹
                        for line in b["lines"]:#b["lines"] è·å–æ–‡æœ¬è¡Œåˆ—è¡¨
                            for span in line["spans"]:  # éå†æ–‡æœ¬ç‰‡æ®µ
                                text_segment = span["text"]
                                full_text += text_segment + " "

                                # è®°å½•æ–‡æœ¬ä½ç½®ï¼ˆç”¨äºæº¯æºé«˜äº®ï¼‰
                                annotations.append({
                                    "text": text_segment,
                                    "page": page_num,  # æ‰€åœ¨é¡µç 
                                    "bbox": span["bbox"]  # åæ ‡[x0,y0,x1,y1]
                                })

            # æå–æ ‡é¢˜ï¼ˆå‰100ä¸ªå­—ç¬¦ï¼‰
            self.title = full_text[:100].replace('\n', ' ').strip()
            if len(self.title) > 97:
                self.title += "..."

            self.text = full_text
            self.annotations = annotations
            return True
        except Exception as e:
            st.error(f"PDFè§£æå¤±è´¥: {str(e)}")
            return False

# æ¨¡å—4ï¼šAIåŠŸèƒ½å‡½æ•°
# ========================
def ai_summarize(text):
    """ç”Ÿæˆæ–‡çŒ®æ€»ç»“"""
    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªå­¦æœ¯ç ”ç©¶åŠ©æ‰‹ï¼Œè¯·ä¸¥æ ¼æ ¹æ®æ–‡çŒ®å†…å®¹å®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼š
    1. ç”¨150å­—ä»¥å†…æ€»ç»“è®ºæ–‡æ ¸å¿ƒè´¡çŒ®å’Œåˆ›æ–°ç‚¹
    2. åˆ—å‡º3-5ä¸ªå…³é”®ä¸“ä¸šæœ¯è¯­åŠç®€æ˜è§£é‡Š
    3. æå‡º2ä¸ªå€¼å¾—æ·±å…¥æ¢è®¨çš„ç ”ç©¶é—®é¢˜

    æ–‡çŒ®å†…å®¹ï¼š
    {text[:15000]}  # é™åˆ¶è¾“å…¥é•¿åº¦
    """

    try:
        response = openai.ChatCompletion.create(
            model="deepseek-chat",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=800,  # é™åˆ¶è¾“å‡ºé•¿åº¦
            temperature=0.3  # é™ä½éšæœºæ€§ï¼Œæé«˜å‡†ç¡®æ€§
        )
        return response.choices[0].message.content
        #responseï¼šå‚¨å­˜APIè°ƒç”¨çš„æ‰€æœ‰ç»“æœï¼›choiceï¼šå‚¨å­˜æ¨¡å‹ç”Ÿæˆçš„æ‰€æœ‰å€™é€‰å“åº”ï¼ˆé€šå¸¸åªæœ‰ä¸€ä¸ªï¼‰ï¼Œä½†ä»ç„¶ä»¥æ•°ç»„å½¢å¼å‚¨å­˜
        #messageï¼šchoiceæ•°ç»„ä¸­å“åº”çš„å…·ä½“è¯¦æƒ…ï¼ˆåŸºäºè§’è‰²+å†…å®¹ï¼‰ï¼›contentï¼šç›´æ¥æå–æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬å†…å®¹
    except Exception as e:
        return f"AIå¤„ç†å¤±è´¥: {str(e)}"
#messages å‚æ•°æ˜¯ OpenAI Chat API ä¸­æœ€æ ¸å¿ƒçš„éƒ¨åˆ†ï¼Œ
#å®ƒå®šä¹‰äº†å¯¹è¯çš„ä¸Šä¸‹æ–‡å’Œå†å²ï¼Œç›´æ¥å†³å®šäº† AI å¦‚ä½•ç†è§£å’Œå“åº”è¯·æ±‚ã€‚

def ai_qa(question, context):
    """é—®ç­”åŠŸèƒ½"""
    prompt = f"""
    è¯·æ ¹æ®ä»¥ä¸‹æ–‡çŒ®å†…å®¹å›ç­”é—®é¢˜ï¼š
    - ç­”æ¡ˆå¿…é¡»æ¥è‡ªæ–‡çŒ®å†…å®¹
    - å¼•ç”¨åŸæ–‡æ—¶æ ‡æ³¨[é¡µç X]
    - å¦‚æ–‡ä¸­æ— ç›¸å…³ä¿¡æ¯ï¼Œè¯·å›ç­”"æ–‡ä¸­æœªæåŠ"

    æ–‡çŒ®æ ‡é¢˜: {processor.title}
    æ–‡çŒ®å†…å®¹: {context[:20000]}  # é™åˆ¶ä¸Šä¸‹æ–‡é•¿åº¦

    é—®é¢˜ï¼š{question}
    """

    try:
        response = openai.ChatCompletion.create(
            model="deepseek-chat",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=500
        )
        return response.choices[0].message.content #response.choicesï¼šåŒ…å«æ‰€æœ‰ç”Ÿæˆé€‰é¡¹çš„åˆ—è¡¨
    except:
        return "AIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åå†è¯•"

# æ¨¡å—5ï¼šå¯è§†åŒ–åŠŸèƒ½
# ========================
def generate_wordcloud(text):
    """ç”Ÿæˆè¯äº‘å›¾"""
    wc = WordCloud(
        width=800,          # å›¾ç‰‡å®½åº¦
        height=400,         # å›¾ç‰‡é«˜åº¦
        background_color='white',  # èƒŒæ™¯è‰²
        max_words=100,      # æœ€å¤šæ˜¾ç¤ºè¯æ•°
        collocations=False,  # ç¦ç”¨è¯ç»„ç»„åˆ
        stopwords=["the", "and", "of", "in", "to", "is"]  # è‡ªå®šä¹‰åœç”¨è¯
    )

    wc.generate(text)  # ä»æ–‡æœ¬ç”Ÿæˆè¯äº‘

    # é…ç½®æ˜¾ç¤º
    plt.figure(figsize=(10, 5))  # è®¾ç½®ç”»å¸ƒå¤§å°
    plt.imshow(wc, interpolation='bilinear')  # å¹³æ»‘æ¸²æŸ“
    plt.axis("off")  # éšè—åæ ‡è½´
    return plt
#========================
def extract_topic_graph(text, top_n=10):#top_nï¼šæå–çš„å…³é”®è¯æ•°é‡ï¼Œé»˜è®¤æ˜¯10ä¸ª
    """
    ä»æ–‡æœ¬ä¸­è‡ªåŠ¨æå–ä¸»é¢˜å…³é”®è¯å¹¶æ„å»º Graphviz å›¾ç»“æ„
    """
    import re #å¯¼å…¥ re æ¨¡å—ç”¨äºæ­£åˆ™è¡¨è¾¾å¼æ¸…æ´—æ–‡æœ¬
    from collections import Counter #å¯¼å…¥ Counter ç”¨äºç»Ÿè®¡è¯é¢‘

    # ç®€å•æ¸…æ´—æ–‡æœ¬
    text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9\s]', '', text)
    # re.sub(pattern, replacement, string)
    # patternï¼šè¦åŒ¹é…çš„æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼ï¼Œr'[^\u4e00-\u9fa5a-zA-Z0-9\s]'æŒ‡åŒ¹é…æ‰€æœ‰ä¸æ˜¯ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—æˆ–ç©ºæ ¼çš„å­—ç¬¦
    # replacementï¼šæ›¿æ¢æˆçš„å†…å®¹ï¼ˆè¿™é‡Œæ˜¯ç©ºå­—ç¬¦ä¸² ''ï¼Œè¡¨ç¤ºåˆ é™¤ã€‚
    # stringï¼šè¦å¤„ç†çš„åŸå§‹æ–‡æœ¬
    words = text.lower().split()
    # å°†æ–‡æœ¬è½¬æ¢ä¸ºå°å†™å¹¶æŒ‰ç©ºæ ¼åˆ†è¯ï¼Œå¾—åˆ°ä¸€ä¸ªè¯åˆ—è¡¨
    # æ’é™¤å¸¸è§åœç”¨è¯
    stopwords = set(["the", "and", "of", "in", "to", "is", "for", "on", "with", "as", "by", "an", "are", "at", "from",
                     "that", "this", "it", "be", "or", "which", "we", "can", "not", "have", "has", "but", "our", "their",
                     "they", "a", "was", "were", "will", "would", "should", "may", "might", "also", "these", "such",
                     "using", "used", "use"])
    keywords = [word for word in words if word not in stopwords and len(word) > 2]

    # ç»Ÿè®¡è¯é¢‘
    freq = Counter(keywords)
    top_keywords = [kw for kw, _ in freq.most_common(top_n)]
    # è¿™æ˜¯ collections.Counter æä¾›çš„æ–¹æ³•ï¼Œç”¨äºè·å–å‡ºç°é¢‘ç‡æœ€é«˜çš„è‹¥å¹²é¡¹
    # éå† freq.most_common(top_n) è¿”å›çš„å‰ top_n ä¸ª (å…³é”®è¯, æ¬¡æ•°) å…ƒç»„ï¼›kw æ˜¯å…³é”®è¯ï¼Œ_ æ˜¯æ¬¡æ•°ï¼ˆç”¨ _ è¡¨ç¤ºæˆ‘ä»¬ä¸å…³å¿ƒå®ƒï¼‰ï¼›
    # æœ€ç»ˆç”Ÿæˆä¸€ä¸ªåªåŒ…å«å…³é”®è¯çš„åˆ—è¡¨ã€‚

    # æ„å»ºç®€å•çš„ä¸»é¢˜å…³ç³»å›¾ï¼ˆçº¿æ€§è¿æ¥ï¼‰
    # ç”¨æ¥æ„å»ºä¸€ä¸ª Graphviz æœ‰å‘å›¾ï¼ˆDOTæ ¼å¼ï¼‰ çš„å­—ç¬¦ä¸²
    dot_code = "digraph {\n"
    #"digraph {\n" æ˜¯ Graphviz çš„è¯­æ³•ï¼Œè¡¨ç¤ºå¼€å§‹å®šä¹‰ä¸€ä¸ª æœ‰å‘å›¾ï¼ˆdirected graphï¼‰ã€‚
    for i in range(len(top_keywords) - 1):
        dot_code += f'"{top_keywords[i]}" -> "{top_keywords[i+1]}"\n'
        #éå†å…³é”®è¯åˆ—è¡¨ top_keywordsï¼Œæ¯æ¬¡å–ä¸¤ä¸ªç›¸é‚»çš„å…³é”®è¯ï¼Œå¹¶ç”Ÿæˆç”±ä¸€ä¸ªæŒ‡å‘å¦ä¸€ä¸ªçš„å›¾è±¡
    dot_code += "}"
    #ç»“æŸå›¾çš„å®šä¹‰ï¼Œè¡¥ä¸Šå³èŠ±æ‹¬å· }ã€‚æœ€ç»ˆdot_code æ˜¯ä¸€ä¸ªå®Œæ•´çš„ Graphviz å›¾ç»“æ„å­—ç¬¦ä¸²ã€‚

    return dot_code

#æ¨¡å—6:æœŸåˆŠæ¨èåŠŸèƒ½
#ç¬¬ä¸€æ­¥ï¼šæŒ‰ç…§ä¹‹å‰æå–å…³é”®è¯å‡½æ•°åŒæ ·çš„æ–‡æœ¬æ¸…æ´—æ–¹æ³•ï¼Œé‡æ–°å®šä¹‰ä¸€ä¸ªç”¨äºarxivæŸ¥è¯¢çš„å‡½æ•°
def get_keywords(text, top_n=5):
    text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9\s]', '', text)
    #æ­£åˆ™è¡¨è¾¾å¼æ¸…æ´—æ–‡æœ¬
    words = text.lower().split()
    stopwords = set(["the", "and", "of", "in", "to", "is", "for", "on", "with", "as", "by", "an", "are", "at", "from",
                     "that", "this", "it", "be", "or", "which", "we", "can", "not", "have", "has", "but", "our", "their",
                     "they", "a", "was", "were", "will", "would", "should", "may", "might", "also", "these", "such",
                     "using", "used", "use"])  # ä¸ extract_topic_graph ä¸­ä¸€è‡´
    keywords = [word for word in words if word not in stopwords and len(word) > 2]
    freq = Counter(keywords)
    return [kw for kw, _ in freq.most_common(top_n)]
#ç¬¬äºŒæ­¥ï¼šæ–°å¢arxivæŸ¥è¯¢å‡½æ•°
def fetch_related_arxiv_papers(keywords, max_results=20):
    query = " OR ".join(keywords)
    search = arxiv.Search(query=query, max_results=max_results, sort_by=arxiv.SortCriterion.Relevance)
    papers = []
    for result in search.results():
        papers.append({
            "title": result.title,
            "summary": result.summary,
            "categories": result.categories
        })
    return papers
#ç¬¬ä¸‰æ­¥ï¼šæ„å»ºä¸€ä¸ªå‡½æ•°ï¼Œå°†æ–‡çŒ®å…³é”®è¯æ˜ å°„åˆ°ä¸åŒæœŸåˆŠä¸Š
def recommend_journals_from_arxiv_categories(text):
    keywords = get_keywords(text)
    papers = fetch_related_arxiv_papers(keywords)

    field_count = Counter()
    for paper in papers:
        for cat in paper["categories"]:
            field_count[cat] += 1

    top_fields = [field for field, _ in field_count.most_common(3)]

    # æ˜ å°„è¡¨ï¼šarXivåˆ†ç±» â†’ æ¨èæœŸåˆŠ
    field_to_journals = {
        "cs.AI": ["Artificial Intelligence", "IEEE Transactions on AI"],
        "stat.ML": ["Journal of Machine Learning Research", "Machine Learning"],
        "cs.CL": ["Computational Linguistics", "ACL", "EMNLP"],
        # å¯ç»§ç»­æ‰©å±•
    }

    recommendations = []
    for field in top_fields:
        journals = field_to_journals.get(field, ["æš‚æ— æ¨è"])
        for journal in journals:
            recommendations.append({"æ¨èæœŸåˆŠ": journal, "ç ”ç©¶æ–¹å‘": field})

    return recommendations

# æ¨¡å—7ï¼šè¯­ä¹‰æœç´¢åŠŸèƒ½
# ========================
@st.cache_resource  # ç¼“å­˜æ¨¡å‹ï¼Œé¿å…é‡å¤åŠ è½½
def load_search_model():
    """åŠ è½½è½»é‡çº§è¯­ä¹‰æœç´¢æ¨¡å‹"""
    return SentenceTransformer('all-MiniLM-L6-v2')  # å°å‹é«˜æ•ˆæ¨¡å‹ï¼Œèƒ½å¤Ÿå°†æ–‡æœ¬è½¬åŒ–æˆå‘é‡

def semantic_search(query, documents, model):
    """æ‰§è¡Œè¯­ä¹‰æœç´¢"""
    query_embed = model.encode([query])  # å°†æŸ¥è¯¢è½¬æ¢ä¸ºå‘é‡
    doc_embeds = model.encode(documents)  # å°†æ–‡æ¡£åˆ—è¡¨è½¬æ¢ä¸ºå‘é‡
    scores = cosine_similarity(query_embed, doc_embeds)[0]  # è®¡ç®—ç›¸ä¼¼åº¦
    return scores

# æ¨¡å—8ï¼šä¸»åº”ç”¨ç•Œé¢
# ========================
def main():
    # åˆå§‹åŒ–ç•Œé¢
    st.title("DSè¾…åŠ©æ–‡çŒ®å¤„ç†ç³»ç»Ÿ")
    st.caption("Demoç‰ˆ - åŸºäºDeepSeekå¤§æ¨¡å‹")

    global processor
    processor = PDFProcessor()  # åˆ›å»ºPDFå¤„ç†å™¨å®ä¾‹

    # åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
    if 'paper_loaded' not in st.session_state:
        st.session_state.paper_loaded = False
        #st.session_state æ˜¯ Streamlit æä¾›çš„çŠ¶æ€ç®¡ç†æœºåˆ¶ã€‚
        #åˆå§‹åŒ–ä¸€ä¸ªå¸ƒå°”å˜é‡ paper_loadedï¼Œè¡¨ç¤ºæ˜¯å¦å·²ç»åŠ è½½äº†æ–‡çŒ®ã€‚

    # ===== ä¾§è¾¹æ  - æ–‡çŒ®ä¸Šä¼ åŒº =====
    with st.sidebar:#è¡¨ç¤ºä»¥ä¸‹å†…å®¹æ˜¾ç¤ºåœ¨ä¾§è¾¹æ ã€‚
        st.header("ğŸ“‚ æ–‡çŒ®ç®¡ç†")#è®¾ç½®ä¾§è¾¹æ æ ‡é¢˜ã€‚
        source_type = st.radio("é€‰æ‹©æ–‡çŒ®æ¥æº", ["ä¸Šä¼ PDF", "arXivè®ºæ–‡ID"])
        #åˆ›å»ºä¸€ä¸ªå•é€‰æ¡†ï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹©ä¸Šä¼ æ–¹å¼ï¼ˆæœ¬åœ° PDF æˆ– arXiv IDï¼‰ã€‚
        # ä¸Šä¼ PDFå¤„ç†
        if source_type == "ä¸Šä¼ PDF":
            uploaded_file = st.file_uploader("é€‰æ‹©PDFæ–‡ä»¶", type="pdf")
            # Streamlit æä¾›çš„æ–‡ä»¶ä¸Šä¼ æ§ä»¶
            #"é€‰æ‹©PDFæ–‡ä»¶" æ˜¯æ§ä»¶ä¸Šæ˜¾ç¤ºçš„æç¤ºæ–‡å­—ã€‚
            #type="pdf" é™åˆ¶ç”¨æˆ·åªèƒ½ä¸Šä¼  .pdf æ–‡ä»¶
            if uploaded_file:
              #uploaded_file æ˜¯é€šè¿‡ st.file_uploader() è·å–çš„ä¸Šä¼ æ–‡ä»¶å¯¹è±¡ã€‚
              #å¦‚æœç”¨æˆ·æ²¡æœ‰ä¸Šä¼ æ–‡ä»¶ï¼Œå®ƒçš„å€¼æ˜¯ Noneï¼Œè¿™è¡Œä»£ç å°±ä¸ä¼šæ‰§è¡Œä¸‹é¢çš„å†…å®¹ã€‚
              #å¦‚æœç”¨æˆ·ä¸Šä¼ äº†æ–‡ä»¶ï¼Œè¿™é‡Œå°±è¿›å…¥å¤„ç†æµç¨‹ã€‚
                with open("temp.pdf", "wb") as f:
                  #Python çš„æ–‡ä»¶æ“ä½œè¯­æ³•ï¼Œæ‰“å¼€ä¸€ä¸ªæ–‡ä»¶å¹¶è‡ªåŠ¨ç®¡ç†å…³é—­
                  #"temp.pdf" æ˜¯ä¿å­˜çš„æ–‡ä»¶åï¼Œè¡¨ç¤ºå°†ä¸Šä¼ çš„ PDF æš‚æ—¶ä¿å­˜ä¸ºè¿™ä¸ªåå­—
                  #"wb" è¡¨ç¤ºä»¥äºŒè¿›åˆ¶å†™å…¥æ¨¡å¼æ‰“å¼€æ–‡ä»¶ï¼ˆPDF æ˜¯äºŒè¿›åˆ¶æ ¼å¼ï¼‰
                    f.write(uploaded_file.getvalue())
                    #uploaded_file.getvalue() è·å–ä¸Šä¼ æ–‡ä»¶çš„å…¨éƒ¨å†…å®¹ï¼ˆå­—èŠ‚æµï¼‰ã€‚
                    #f.write(...) å°†è¿™äº›å†…å®¹å†™å…¥æœ¬åœ°æ–‡ä»¶ã€‚
                if processor.parse_pdf("temp.pdf"):
                  # è°ƒç”¨ PDFProcessor ç±»ä¸­çš„ parse_pdf() æ–¹æ³•ï¼Œä¼ å…¥åˆšä¿å­˜çš„ "temp.pdf" æ–‡ä»¶è·¯å¾„ã€‚
                  #å¦‚æœè§£ææˆåŠŸåˆ™è¿”å›Trueï¼Œå¹¶ä¸”è¿›è¡Œä¸‹ä¸€æ­¥
                    st.session_state.paper_loaded = True
                    st.success(f"å·²åŠ è½½: {processor.title}")

        # arXivè®ºæ–‡å¤„ç†
        else:
            arxiv_id = st.text_input("è¾“å…¥arXiv ID (å¦‚ 2305.10403)")
            if arxiv_id:
                try:
                    search = arxiv.Search(id_list=[arxiv_id])  # æœç´¢è®ºæ–‡
                    paper = next(search.results())
                    #search.results() è¿”å›ä¸€ä¸ªç”Ÿæˆå™¨ï¼ˆè®ºæ–‡æœç´¢ç»“æœï¼‰ã€‚
                    #next(...) è·å–ç¬¬ä¸€ç¯‡è®ºæ–‡ï¼ˆé€šå¸¸å°±æ˜¯å”¯ä¸€çš„ä¸€ç¯‡ï¼‰ã€‚
                    paper.download_pdf(filename="arxiv_paper.pdf")
                    #download_pdfæ˜¯ arxiv.Result ç±»ä¸­çš„ä¸€ä¸ªæ–¹æ³•ï¼Œç”¨äºä¸‹è½½ï¼Œè€Œä¸‹è½½å¯¹è±¡æ˜¯paper
                    if processor.parse_pdf("arxiv_paper.pdf"):  # è§£æPDF
                        st.session_state.paper_loaded = True
                        st.success(f"å·²åŠ è½½: {paper.title}")
                except Exception as e:
                    st.error(f"è®ºæ–‡è·å–å¤±è´¥: {str(e)}")

#===== ä¸»åŠŸèƒ½åŒºåŸŸ - æ ‡ç­¾é¡µ =====
tab1, tab2, tab3 = st.tabs(["æ–‡çŒ®è§£æ", "è¯­ä¹‰æœç´¢", "æœŸåˆŠæ¨è"])
    #ä½¿ç”¨ Streamlit æ¡†æ¶ä¸­çš„ st.tabs() æ–¹æ³•æ¥åˆ›å»ºä¸€ä¸ª å¤šæ ‡ç­¾é¡µç•Œé¢

# æ ‡ç­¾é¡µ1ï¼šæ–‡çŒ®è§£æ
with tab1:
    if st.session_state.paper_loaded:
        st.subheader(f"æ–‡çŒ®åˆ†æ: {processor.title}")
        #æ˜¾ç¤ºæ–‡çŒ®æ ‡é¢˜ä½œä¸ºå­æ ‡é¢˜
        # 1. AIæ€»ç»“åŠŸèƒ½
        with st.expander("AIæ™ºèƒ½æ€»ç»“", expanded=True):
            #åˆ›å»ºä¸€ä¸ªå¯å±•å¼€åŒºåŸŸï¼Œé»˜è®¤å±•å¼€ã€‚ç”¨äºå®¹çº³ AI æ€»ç»“æŒ‰é’®å’Œè¾“å‡ºç»“æœ
            if st.button("ç”Ÿæˆæ–‡çŒ®æ€»ç»“", key="summarize_btn"):
                #å‚æ•° key="summarize_btn" çš„ä½œç”¨æ˜¯ä¸ºè¿™ä¸ªæŒ‰é’®ç»„ä»¶æŒ‡å®šä¸€ä¸ª å”¯ä¸€æ ‡è¯†ç¬¦ã€‚
                #è¿™æ˜¯ Streamlit ä¸­éå¸¸é‡è¦çš„æœºåˆ¶ï¼šå¯ä»¥ç”Ÿæˆå”¯ä¸€æ ‡è¯†ï¼Œé˜²æ­¢ç»„ä»¶å†²çªï¼ŒåŒæ—¶å¯ä»¥è·Ÿè¸ªç”¨æˆ·äº¤äº’çŠ¶æ€ï¼ˆä¾‹å¦‚æ˜¯å¦ç‚¹å‡»äº†æŒ‰é’®ï¼‰
                with st.spinner("AIæ­£åœ¨åˆ†ææ–‡çŒ®å†…å®¹..."):
                    #æ˜¾ç¤ºä¸€ä¸ªåŠ è½½åŠ¨ç”»ï¼Œæç¤ºç”¨æˆ· AI æ­£åœ¨å¤„ç†
                    summary = ai_summarize(processor.text)
                    st.markdown(summary)

        # 2. è¿½é—®åŠŸèƒ½
        st.subheader("æ–‡çŒ®é—®ç­”")
        question = st.text_input("è¾“å…¥å…³äºè¿™ç¯‡æ–‡çŒ®çš„é—®é¢˜", placeholder="æœ¬æ–‡çš„åˆ›æ–°ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ")
        if question:
            with st.spinner("æ­£åœ¨æŸ¥æ‰¾ç­”æ¡ˆ..."):
                answer = ai_qa(question, processor.text)
                st.markdown(f"**AIå›ç­”**: {answer}")

        # 3. å¯è§†åŒ–åˆ†æ
        col1, col2 = st.columns([1, 1])
        #ä½¿ç”¨ Streamlit çš„ columns æ–¹æ³•å°†é¡µé¢åˆ†ä¸ºå·¦å³ä¸¤åˆ—ï¼Œæ¯”ä¾‹ä¸º 1:1ã€‚
        #è¿™æ ·å¯ä»¥åœ¨åŒä¸€è¡Œä¸­å¹¶æ’å±•ç¤ºä¸¤ä¸ªå›¾è¡¨ï¼šè¯äº‘å›¾å’Œä¸»é¢˜å…³ç³»å›¾ã€‚
        with col1:
            st.subheader("ç ”ç©¶çƒ­ç‚¹è¯äº‘")
            st.pyplot(generate_wordcloud(processor.text))
            #st.pyplot(...) ç”¨äºæ˜¾ç¤ºç”± generate_wordcloud() å‡½æ•°ç”Ÿæˆçš„è¯äº‘å›¾ã€‚
            #generate_wordcloud(processor.text) è¢«è°ƒç”¨ï¼Œè¿”å›çš„æ˜¯ plt å¯¹è±¡ï¼Œå…¶ä¸­å·²ç»åŒ…å«äº†è¯äº‘å›¾çš„ç»˜åˆ¶å†…å®¹ã€‚
            #st.pyplot(...) æ˜¯ Streamlit çš„æ–¹æ³•ï¼Œç”¨äºå°† matplotlib å›¾åƒåµŒå…¥åˆ°ç½‘é¡µä¸­ã€‚
        with col2:
            st.subheader("ä¸»é¢˜å…³ç³»å›¾")
            st.caption("ç¤ºä¾‹å›¾ - åŸºäºæ–‡çŒ®å†…å®¹ç”Ÿæˆ")
            #st.caption(...) æ·»åŠ è¯´æ˜æ–‡å­—ï¼Œæç¤ºè¿™æ˜¯ä¸€ä¸ªç¤ºä¾‹å›¾
            # ä½¿ç”¨Graphvizç”Ÿæˆç®€å•ä¸»é¢˜å…³ç³»å›¾
            dot_code = extract_topic_graph(processor.text)
            st.graphviz_chart(dot_code)

# æ ‡ç­¾é¡µ2ï¼šè¯­ä¹‰æœç´¢
with tab2:
    st.subheader("è¯­ä¹‰æ–‡çŒ®æœç´¢")
    query = st.text_input("è¾“å…¥æœç´¢å…³é”®è¯", key="search_query")
    if query:
        model = load_search_model()  # åŠ è½½è¯­ä¹‰æ¨¡å‹
        real_papers = fetch_arxiv_titles(query=query, max_results=10)
        scores = semantic_search(query, real_papers, model)  # æ‰§è¡Œæœç´¢
        #è°ƒç”¨ semantic_search() å‡½æ•°ï¼Œä½¿ç”¨è¯­ä¹‰æ¨¡å‹å°†ç”¨æˆ·è¾“å…¥çš„æŸ¥è¯¢ä¸æ–‡çŒ®åº“ä¸­çš„æ¯ç¯‡æ–‡çŒ®è¿›è¡Œå‘é‡åŒ–å¹¶è®¡ç®—ç›¸ä¼¼åº¦åˆ†æ•°ã€‚
        st.write("### æœç´¢ç»“æœæ’åº:")
        results = sorted(zip(real_papers, scores), key=lambda x: x[1], reverse=True)
        #zip(real_papers, scores)ä½œç”¨ï¼šå°†ä¸¤ä¸ªåˆ—è¡¨ demo_papers å’Œ scores ç»„åˆæˆä¸€ä¸ªå…ƒç»„åˆ—è¡¨ã€‚
        #key=lambda x: x[1] è¡¨ç¤ºæŒ‰ç…§æ¯ä¸ªå…ƒç»„çš„ç¬¬äºŒä¸ªå…ƒç´ ï¼ˆå³ç›¸ä¼¼åº¦åˆ†æ•°ï¼‰è¿›è¡Œæ’åºã€‚
        # æ˜¾ç¤ºå‰ä¸‰ç»“æœ
        for i, (paper, score) in enumerate(results[:3]):
            #enumerate() æ˜¯ Python çš„å†…ç½®å‡½æ•°ï¼Œç”¨äºåœ¨éå†å¯è¿­ä»£å¯¹è±¡æ—¶åŒæ—¶è·å–ç´¢å¼•å’Œå€¼
            #éœ€è¦æ˜¾ç¤ºç¼–å·æ—¶enumerateæ¯”è¾ƒæ–¹ä¾¿
            st.markdown(f"**{i+1}. {paper}**")
            #i+1 æ˜¯æ–‡çŒ®çš„æ’åç¼–å·ï¼ˆä» 1 å¼€å§‹ï¼‰ã€‚paper æ˜¯æ–‡çŒ®çš„æ ‡é¢˜å­—ç¬¦ä¸²ã€‚
            #ä¾‹å¦‚ï¼š**1. æ·±åº¦å­¦ä¹ ä¼˜åŒ–ç®—æ³•çš„æœ€æ–°è¿›å±•**
            st.progress(float(score), text=f"ç›¸å…³åº¦: {score:.2f}")
            #æ˜¾ç¤ºä¸€ä¸ªè¿›åº¦æ¡ï¼Œè¡¨ç¤ºè¯¥æ–‡çŒ®ä¸ç”¨æˆ·æœç´¢å…³é”®è¯çš„è¯­ä¹‰ç›¸å…³åº¦ã€‚

# æ ‡ç­¾é¡µ3ï¼šæœŸåˆŠæ¨è
with tab3:
    st.subheader("æœŸåˆŠæŠ•é€’å»ºè®®")
    if st.session_state.paper_loaded:
        # ç®€åŒ–çš„æŸ¥é‡é€»è¾‘ï¼ˆå®é™…åº”ç”¨éœ€æ›´å¤æ‚å®ç°ï¼‰
        recommendations = recommend_journals_from_arxiv_categories(processor.text)
        st.subheader("æ¨èæœŸåˆŠï¼ˆåŸºäºarXivåˆ†ç±»ï¼‰")
        st.table(recommendations)
    else:
        st.warning("è¯·å…ˆåŠ è½½æ–‡çŒ®")

# é¡µè„šä¿¡æ¯
    st.divider()
    st.caption("è¯¾ç¨‹é¡¹ç›®Demo | åŸºäºStreamlit+DeepSeekæ„å»º | ä»…ç”¨äºå­¦æœ¯æ¼”ç¤º")

# ========================
# æ¨¡å—8ï¼šç¨‹åºå¯åŠ¨
# ========================
if __name__ == "__main__":
    processor = PDFProcessor()  # åˆ›å»ºå…¨å±€PDFå¤„ç†å™¨å®ä¾‹
    main()  # å¯åŠ¨ä¸»åº”ç”¨



