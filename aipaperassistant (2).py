# -*- coding: utf-8 -*-
"""aipaperassistant.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1x2tVhpwBDr-cE96D4BaDyIU2nbiRXPuB
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile aipaperassistant.py

# 模块1：导入依赖库
# ========================
#1.1streamlit、pymupdf、arxiv安装和环境配置
!pip install streamlit PyMuPDF arxiv
# 安装虚拟显示支持（Colab没有图形界面）
!apt-get install -y xvfb
!pip install pyvirtualdisplay
# 设置虚拟显示
from pyvirtualdisplay import Display
display = Display(visible=0, size=(1024, 768))
display.start()
import streamlit as st  # 网页应用框架
import fitz  # PyMuPDF库，用于PDF解析
import numpy as np  # 数值计算
import openai  # 大模型API接口
import os  # 操作系统接口
import re  # 正则表达式
import time  # 时间处理
from wordcloud import WordCloud  # 词云生成
import matplotlib.pyplot as plt  # 数据可视化
from sentence_transformers import SentenceTransformer  # 语义搜索模型
from sklearn.metrics.pairwise import cosine_similarity  # 相似度计算
import arxiv  # arXiv论文获取

# 模块2：初始化设置
# ========================
# 配置Streamlit页面
st.set_page_config(
    page_title="📚 AI文献阅读系统",
    page_icon="📖",
    layout="wide")
# 在streamlit配置DeepSeek API
openai.api_key = st.secrets["DEEPSEEK_API_KEY"]

# 模块3：PDF解析类
# ========================
class PDFProcessor:
    """处理PDF文件，提取文本和位置信息"""
    def __init__(self):
        self.text = ""
        self.annotations = []
        self.title = "未命名文献"

    def parse_pdf(self, file_path):
        """解析PDF文本和位置信息"""
        try:
            doc = fitz.open(file_path)  # 打开PDF文件
            full_text = ""
            annotations = []

            # 逐页解析
            for page_num in range(len(doc)):
                page = doc.load_page(page_num)  # 加载当前页
                blocks = page.get_text("dict").get("blocks", [])  # 获取文本块

                # 遍历文本块
                #PDF文本有层级结构：块(block) → 行(line) → 片段(span)
                for b in blocks:
                    if "lines" in b:  # 确认是文本行（每个文本块blocks包含自己的lines列表）
                    #排除图像块、空白块等非文本内容
                        for line in b["lines"]:#b["lines"] 获取文本行列表
                            for span in line["spans"]:  # 遍历文本片段
                                text_segment = span["text"]
                                full_text += text_segment + " "

                                # 记录文本位置（用于溯源高亮）
                                annotations.append({
                                    "text": text_segment,
                                    "page": page_num,  # 所在页码
                                    "bbox": span["bbox"]  # 坐标[x0,y0,x1,y1]
                                })

            # 提取标题（前100个字符）
            self.title = full_text[:100].replace('\n', ' ').strip()
            if len(self.title) > 97:
                self.title += "..."

            self.text = full_text
            self.annotations = annotations
            return True
        except Exception as e:
            st.error(f"PDF解析失败: {str(e)}")
            return False

# 模块4：AI功能函数
# ========================
def ai_summarize(text):
    """生成文献总结"""
    prompt = f"""
    你是一个学术研究助手，请严格根据文献内容完成以下任务：
    1. 用150字以内总结论文核心贡献和创新点
    2. 列出3-5个关键专业术语及简明解释
    3. 提出2个值得深入探讨的研究问题

    文献内容：
    {text[:15000]}  # 限制输入长度
    """

    try:
        response = openai.ChatCompletion.create(
            model="deepseek-chat",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=800,  # 限制输出长度
            temperature=0.3  # 降低随机性，提高准确性
        )
        return response.choices[0].message.content
        #response：储存API调用的所有结果；choice：储存模型生成的所有候选响应（通常只有一个），但仍然以数组形式储存
        #message：choice数组中响应的具体详情（基于角色+内容）；content：直接提取模型生成的文本内容
    except Exception as e:
        return f"AI处理失败: {str(e)}"
#messages 参数是 OpenAI Chat API 中最核心的部分，
#它定义了对话的上下文和历史，直接决定了 AI 如何理解和响应请求。

def ai_qa(question, context):
    """问答功能"""
    prompt = f"""
    请根据以下文献内容回答问题：
    - 答案必须来自文献内容
    - 引用原文时标注[页码X]
    - 如文中无相关信息，请回答"文中未提及"

    文献标题: {processor.title}
    文献内容: {context[:20000]}  # 限制上下文长度

    问题：{question}
    """

    try:
        response = openai.ChatCompletion.create(
            model="deepseek-chat",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=500
        )
        return response.choices[0].message.content #response.choices：包含所有生成选项的列表
    except:
        return "AI服务暂时不可用，请稍后再试"

# 模块5：可视化功能
# ========================
def generate_wordcloud(text):
    """生成词云图"""
    wc = WordCloud(
        width=800,          # 图片宽度
        height=400,         # 图片高度
        background_color='white',  # 背景色
        max_words=100,      # 最多显示词数
        collocations=False,  # 禁用词组组合
        stopwords=["the", "and", "of", "in", "to", "is"]  # 自定义停用词
    )

    wc.generate(text)  # 从文本生成词云

    # 配置显示
    plt.figure(figsize=(10, 5))  # 设置画布大小
    plt.imshow(wc, interpolation='bilinear')  # 平滑渲染
    plt.axis("off")  # 隐藏坐标轴
    return plt
#========================
def extract_topic_graph(text, top_n=10):#top_n：提取的关键词数量，默认是10个
    """
    从文本中自动提取主题关键词并构建 Graphviz 图结构
    """
    import re #导入 re 模块用于正则表达式清洗文本
    from collections import Counter #导入 Counter 用于统计词频

    # 简单清洗文本
    text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9\s]', '', text)
    # re.sub(pattern, replacement, string)
    # pattern：要匹配的正则表达式模式，r'[^\u4e00-\u9fa5a-zA-Z0-9\s]'指匹配所有不是中文、英文、数字或空格的字符
    # replacement：替换成的内容（这里是空字符串 ''，表示删除。
    # string：要处理的原始文本
    words = text.lower().split()
    # 将文本转换为小写并按空格分词，得到一个词列表
    # 排除常见停用词
    stopwords = set(["the", "and", "of", "in", "to", "is", "for", "on", "with", "as", "by", "an", "are", "at", "from",
                     "that", "this", "it", "be", "or", "which", "we", "can", "not", "have", "has", "but", "our", "their",
                     "they", "a", "was", "were", "will", "would", "should", "may", "might", "also", "these", "such",
                     "using", "used", "use"])
    keywords = [word for word in words if word not in stopwords and len(word) > 2]

    # 统计词频
    freq = Counter(keywords)
    top_keywords = [kw for kw, _ in freq.most_common(top_n)]
    # 这是 collections.Counter 提供的方法，用于获取出现频率最高的若干项
    # 遍历 freq.most_common(top_n) 返回的前 top_n 个 (关键词, 次数) 元组；kw 是关键词，_ 是次数（用 _ 表示我们不关心它）；
    # 最终生成一个只包含关键词的列表。

    # 构建简单的主题关系图（线性连接）
    # 用来构建一个 Graphviz 有向图（DOT格式） 的字符串
    dot_code = "digraph {\n"
    #"digraph {\n" 是 Graphviz 的语法，表示开始定义一个 有向图（directed graph）。
    for i in range(len(top_keywords) - 1):
        dot_code += f'"{top_keywords[i]}" -> "{top_keywords[i+1]}"\n'
        #遍历关键词列表 top_keywords，每次取两个相邻的关键词，并生成由一个指向另一个的图象
    dot_code += "}"
    #结束图的定义，补上右花括号 }。最终dot_code 是一个完整的 Graphviz 图结构字符串。

    return dot_code

#模块6:期刊推荐功能
#第一步：按照之前提取关键词函数同样的文本清洗方法，重新定义一个用于arxiv查询的函数
def get_keywords(text, top_n=5):
    text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9\s]', '', text)
    #正则表达式清洗文本
    words = text.lower().split()
    stopwords = set(["the", "and", "of", "in", "to", "is", "for", "on", "with", "as", "by", "an", "are", "at", "from",
                     "that", "this", "it", "be", "or", "which", "we", "can", "not", "have", "has", "but", "our", "their",
                     "they", "a", "was", "were", "will", "would", "should", "may", "might", "also", "these", "such",
                     "using", "used", "use"])  # 与 extract_topic_graph 中一致
    keywords = [word for word in words if word not in stopwords and len(word) > 2]
    freq = Counter(keywords)
    return [kw for kw, _ in freq.most_common(top_n)]
#第二步：新增arxiv查询函数
def fetch_related_arxiv_papers(keywords, max_results=20):
    query = " OR ".join(keywords)
    search = arxiv.Search(query=query, max_results=max_results, sort_by=arxiv.SortCriterion.Relevance)
    papers = []
    for result in search.results():
        papers.append({
            "title": result.title,
            "summary": result.summary,
            "categories": result.categories
        })
    return papers
#第三步：构建一个函数，将文献关键词映射到不同期刊上
def recommend_journals_from_arxiv_categories(text):
    keywords = get_keywords(text)
    papers = fetch_related_arxiv_papers(keywords)

    field_count = Counter()
    for paper in papers:
        for cat in paper["categories"]:
            field_count[cat] += 1

    top_fields = [field for field, _ in field_count.most_common(3)]

    # 映射表：arXiv分类 → 推荐期刊
    field_to_journals = {
        "cs.AI": ["Artificial Intelligence", "IEEE Transactions on AI"],
        "stat.ML": ["Journal of Machine Learning Research", "Machine Learning"],
        "cs.CL": ["Computational Linguistics", "ACL", "EMNLP"],
        # 可继续扩展
    }

    recommendations = []
    for field in top_fields:
        journals = field_to_journals.get(field, ["暂无推荐"])
        for journal in journals:
            recommendations.append({"推荐期刊": journal, "研究方向": field})

    return recommendations

# 模块7：语义搜索功能
# ========================
@st.cache_resource  # 缓存模型，避免重复加载
def load_search_model():
    """加载轻量级语义搜索模型"""
    return SentenceTransformer('all-MiniLM-L6-v2')  # 小型高效模型，能够将文本转化成向量

def semantic_search(query, documents, model):
    """执行语义搜索"""
    query_embed = model.encode([query])  # 将查询转换为向量
    doc_embeds = model.encode(documents)  # 将文档列表转换为向量
    scores = cosine_similarity(query_embed, doc_embeds)[0]  # 计算相似度
    return scores

# 模块8：主应用界面
# ========================
def main():
    # 初始化界面
    st.title("DS辅助文献处理系统")
    st.caption("Demo版 - 基于DeepSeek大模型")

    global processor
    processor = PDFProcessor()  # 创建PDF处理器实例

    # 初始化会话状态
    if 'paper_loaded' not in st.session_state:
        st.session_state.paper_loaded = False
        #st.session_state 是 Streamlit 提供的状态管理机制。
        #初始化一个布尔变量 paper_loaded，表示是否已经加载了文献。

    # ===== 侧边栏 - 文献上传区 =====
    with st.sidebar:#表示以下内容显示在侧边栏。
        st.header("📂 文献管理")#设置侧边栏标题。
        source_type = st.radio("选择文献来源", ["上传PDF", "arXiv论文ID"])
        #创建一个单选框，用户可以选择上传方式（本地 PDF 或 arXiv ID）。
        # 上传PDF处理
        if source_type == "上传PDF":
            uploaded_file = st.file_uploader("选择PDF文件", type="pdf")
            # Streamlit 提供的文件上传控件
            #"选择PDF文件" 是控件上显示的提示文字。
            #type="pdf" 限制用户只能上传 .pdf 文件
            if uploaded_file:
              #uploaded_file 是通过 st.file_uploader() 获取的上传文件对象。
              #如果用户没有上传文件，它的值是 None，这行代码就不会执行下面的内容。
              #如果用户上传了文件，这里就进入处理流程。
                with open("temp.pdf", "wb") as f:
                  #Python 的文件操作语法，打开一个文件并自动管理关闭
                  #"temp.pdf" 是保存的文件名，表示将上传的 PDF 暂时保存为这个名字
                  #"wb" 表示以二进制写入模式打开文件（PDF 是二进制格式）
                    f.write(uploaded_file.getvalue())
                    #uploaded_file.getvalue() 获取上传文件的全部内容（字节流）。
                    #f.write(...) 将这些内容写入本地文件。
                if processor.parse_pdf("temp.pdf"):
                  # 调用 PDFProcessor 类中的 parse_pdf() 方法，传入刚保存的 "temp.pdf" 文件路径。
                  #如果解析成功则返回True，并且进行下一步
                    st.session_state.paper_loaded = True
                    st.success(f"已加载: {processor.title}")

        # arXiv论文处理
        else:
            arxiv_id = st.text_input("输入arXiv ID (如 2305.10403)")
            if arxiv_id:
                try:
                    search = arxiv.Search(id_list=[arxiv_id])  # 搜索论文
                    paper = next(search.results())
                    #search.results() 返回一个生成器（论文搜索结果）。
                    #next(...) 获取第一篇论文（通常就是唯一的一篇）。
                    paper.download_pdf(filename="arxiv_paper.pdf")
                    #download_pdf是 arxiv.Result 类中的一个方法，用于下载，而下载对象是paper
                    if processor.parse_pdf("arxiv_paper.pdf"):  # 解析PDF
                        st.session_state.paper_loaded = True
                        st.success(f"已加载: {paper.title}")
                except Exception as e:
                    st.error(f"论文获取失败: {str(e)}")

#===== 主功能区域 - 标签页 =====
tab1, tab2, tab3 = st.tabs(["文献解析", "语义搜索", "期刊推荐"])
    #使用 Streamlit 框架中的 st.tabs() 方法来创建一个 多标签页界面

# 标签页1：文献解析
with tab1:
    if st.session_state.paper_loaded:
        st.subheader(f"文献分析: {processor.title}")
        #显示文献标题作为子标题
        # 1. AI总结功能
        with st.expander("AI智能总结", expanded=True):
            #创建一个可展开区域，默认展开。用于容纳 AI 总结按钮和输出结果
            if st.button("生成文献总结", key="summarize_btn"):
                #参数 key="summarize_btn" 的作用是为这个按钮组件指定一个 唯一标识符。
                #这是 Streamlit 中非常重要的机制：可以生成唯一标识，防止组件冲突，同时可以跟踪用户交互状态（例如是否点击了按钮）
                with st.spinner("AI正在分析文献内容..."):
                    #显示一个加载动画，提示用户 AI 正在处理
                    summary = ai_summarize(processor.text)
                    st.markdown(summary)

        # 2. 追问功能
        st.subheader("文献问答")
        question = st.text_input("输入关于这篇文献的问题", placeholder="本文的创新点是什么？")
        if question:
            with st.spinner("正在查找答案..."):
                answer = ai_qa(question, processor.text)
                st.markdown(f"**AI回答**: {answer}")

        # 3. 可视化分析
        col1, col2 = st.columns([1, 1])
        #使用 Streamlit 的 columns 方法将页面分为左右两列，比例为 1:1。
        #这样可以在同一行中并排展示两个图表：词云图和主题关系图。
        with col1:
            st.subheader("研究热点词云")
            st.pyplot(generate_wordcloud(processor.text))
            #st.pyplot(...) 用于显示由 generate_wordcloud() 函数生成的词云图。
            #generate_wordcloud(processor.text) 被调用，返回的是 plt 对象，其中已经包含了词云图的绘制内容。
            #st.pyplot(...) 是 Streamlit 的方法，用于将 matplotlib 图像嵌入到网页中。
        with col2:
            st.subheader("主题关系图")
            st.caption("示例图 - 基于文献内容生成")
            #st.caption(...) 添加说明文字，提示这是一个示例图
            # 使用Graphviz生成简单主题关系图
            dot_code = extract_topic_graph(processor.text)
            st.graphviz_chart(dot_code)

# 标签页2：语义搜索
with tab2:
    st.subheader("语义文献搜索")
    query = st.text_input("输入搜索关键词", key="search_query")
    if query:
        model = load_search_model()  # 加载语义模型
        real_papers = fetch_arxiv_titles(query=query, max_results=10)
        scores = semantic_search(query, real_papers, model)  # 执行搜索
        #调用 semantic_search() 函数，使用语义模型将用户输入的查询与文献库中的每篇文献进行向量化并计算相似度分数。
        st.write("### 搜索结果排序:")
        results = sorted(zip(real_papers, scores), key=lambda x: x[1], reverse=True)
        #zip(real_papers, scores)作用：将两个列表 demo_papers 和 scores 组合成一个元组列表。
        #key=lambda x: x[1] 表示按照每个元组的第二个元素（即相似度分数）进行排序。
        # 显示前三结果
        for i, (paper, score) in enumerate(results[:3]):
            #enumerate() 是 Python 的内置函数，用于在遍历可迭代对象时同时获取索引和值
            #需要显示编号时enumerate比较方便
            st.markdown(f"**{i+1}. {paper}**")
            #i+1 是文献的排名编号（从 1 开始）。paper 是文献的标题字符串。
            #例如：**1. 深度学习优化算法的最新进展**
            st.progress(float(score), text=f"相关度: {score:.2f}")
            #显示一个进度条，表示该文献与用户搜索关键词的语义相关度。

# 标签页3：期刊推荐
with tab3:
    st.subheader("期刊投递建议")
    if st.session_state.paper_loaded:
        # 简化的查重逻辑（实际应用需更复杂实现）
        recommendations = recommend_journals_from_arxiv_categories(processor.text)
        st.subheader("推荐期刊（基于arXiv分类）")
        st.table(recommendations)
    else:
        st.warning("请先加载文献")

# 页脚信息
    st.divider()
    st.caption("课程项目Demo | 基于Streamlit+DeepSeek构建 | 仅用于学术演示")

# ========================
# 模块8：程序启动
# ========================
if __name__ == "__main__":
    processor = PDFProcessor()  # 创建全局PDF处理器实例
    main()  # 启动主应用



